{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth-Anything-V2-Mini Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Depth-Anything-V2 with Test Image\n",
    "- [Reference: Hugging Face](https://huggingface.co/docs/transformers/main/model_doc/depth_anything_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianruzhang/anaconda3/envs/cv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image processor and model\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"depth-anything/Depth-Anything-V2-Small-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor: torch.Size([1, 3, 518, 672])\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"test.jpg\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "tensor_shape = inputs['pixel_values'].shape\n",
    "print(f\"Shape of the tensor: {tensor_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "depth_map = image_processor.post_process_depth_estimation(outputs, target_sizes=[image.size[::-1]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianruzhang/anaconda3/envs/cv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import util.image as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import NYUDepthV2Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = NYUDepthV2Dataset('nyu_depth_v2_labeled.mat', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([16, 3, 240, 320])\n",
      "Depths shape: torch.Size([16, 240, 320])\n"
     ]
    }
   ],
   "source": [
    "for images, depths in dataloader:\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Depths shape:\", depths.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the NYU v2 Dataset\n",
    "- Importing Dataset from Hugging Face: [Hugging Face: NYU Depth V2](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2)\n",
    "- Downloading Dataset: [NYU Depth V2](https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(dataset.file['images'])\n",
    "depths = np.array(dataset.file['depths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(len(images), 9).tolist()\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    image_viz = u.merge_into_row(images[idx], depths[idx])\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(image_viz.astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model \n",
    "- Importing Depth-Anything-V2 Small from Transformer: [Hugging Face: Depth Anything V2](https://huggingface.co/docs/transformers/main/en/model_doc/depth_anything_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Baseline as b\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
       "========================================================================================================================\n",
       "DepthAnythingBaseline                                                  [16, 240, 320]            --\n",
       "├─DepthAnythingForDepthEstimation: 1-1                                 [16, 238, 308]            --\n",
       "│    └─Dinov2Backbone: 2-1                                             [16, 375, 384]            --\n",
       "│    │    └─Dinov2Embeddings: 3-1                                      [16, 375, 384]            753,024\n",
       "│    │    └─Dinov2Encoder: 3-2                                         [16, 375, 384]            21,302,784\n",
       "│    │    └─LayerNorm: 3-3                                             [16, 375, 384]            768\n",
       "│    │    └─LayerNorm: 3-4                                             [16, 375, 384]            (recursive)\n",
       "│    │    └─LayerNorm: 3-5                                             [16, 375, 384]            (recursive)\n",
       "│    │    └─LayerNorm: 3-6                                             [16, 375, 384]            (recursive)\n",
       "│    └─DepthAnythingNeck: 2-2                                          [16, 64, 17, 22]          --\n",
       "│    │    └─DepthAnythingReassembleStage: 3-7                          [16, 48, 68, 88]          1,678,560\n",
       "│    │    └─ModuleList: 3-8                                            --                        414,720\n",
       "│    │    └─DepthAnythingFeatureFusionStage: 3-9                       [16, 64, 17, 22]          607,488\n",
       "│    └─DepthAnythingDepthEstimationHead: 2-3                           [16, 238, 308]            --\n",
       "│    │    └─Conv2d: 3-10                                               [16, 32, 136, 176]        18,464\n",
       "│    │    └─Conv2d: 3-11                                               [16, 32, 238, 308]        9,248\n",
       "│    │    └─ReLU: 3-12                                                 [16, 32, 238, 308]        --\n",
       "│    │    └─Conv2d: 3-13                                               [16, 1, 238, 308]         33\n",
       "│    │    └─ReLU: 3-14                                                 [16, 1, 238, 308]         --\n",
       "========================================================================================================================\n",
       "Total params: 24,785,089\n",
       "Trainable params: 24,785,089\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 53.61\n",
       "========================================================================================================================\n",
       "Input size (MB): 14.75\n",
       "Forward/backward pass size (MB): 4054.22\n",
       "Params size (MB): 96.74\n",
       "Estimated Total Size (MB): 4165.70\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, criterion, optimizer = b.create_baseline()\n",
    "summary(model, input_size=(16, 3, 240, 320))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.eval as e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Avg Loss: 1.9267, δ1 Accuracy with 1.25 threshold: 15.48%, δ1 Accuracy with 1.50 threshold: 27.30%, MAE: 1.9267\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "loss_values = []\n",
    "delta_accuracy_values_50 = []\n",
    "delta_accuracy_values_25 = []\n",
    "mae_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_delta_accuracy_50 = 0\n",
    "    total_delta_accuracy_25 = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    for images, depths in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "        images, depths = images.to(device), depths.to(device)\n",
    "                \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, depths)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        with torch.no_grad():\n",
    "            delta_accuracy_50 = e.compute_delta_accuracy(outputs, depths, 1.5)\n",
    "            delta_accuracy_25 = e.compute_delta_accuracy(outputs, depths, 1.25)\n",
    "            mae = torch.abs(outputs - depths).mean().item()\n",
    "            \n",
    "            total_delta_accuracy_50 += delta_accuracy_50\n",
    "            total_delta_accuracy_25 += delta_accuracy_25\n",
    "            total_mae += mae\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_delta_accuracy_50 = total_delta_accuracy_50 / len(dataloader)\n",
    "    avg_delta_accuracy_25 = total_delta_accuracy_25 / len(dataloader)\n",
    "    avg_mae = total_mae / len(dataloader)\n",
    "    \n",
    "    loss_values.append(avg_loss)\n",
    "    delta_accuracy_values_50.append(avg_delta_accuracy_50)\n",
    "    delta_accuracy_values_25.append(avg_delta_accuracy_25)\n",
    "    mae_values.append(avg_mae)\n",
    "    \n",
    "    tqdm.write(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, '\n",
    "               f'δ1 Accuracy with 1.25 threshold: {avg_delta_accuracy_25:.2f}%, '\n",
    "               f'δ1 Accuracy with 1.50 threshold: {avg_delta_accuracy_50:.2f}%, '\n",
    "               f'MAE: {avg_mae:.4f}')\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
